{
    "contents" : "---\ntitle: \"Определение тональности обзоров фильмов\"\nauthor: \"Морозов Глеб\"\ndate: \"30 августа 2015 г.\"\noutput: \n  html_document: \n    keep_md: yes\n---\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(warning=FALSE, message=FALSE)\n```\n\nВ данной работе рассматриваются различные возможности применения инструментов предоставляемых языком R для определения тональности обзоров фильмов.\n\n### Данные\n\nДанные для работы предоставлены в рамках соревнования [Bag of Words](https://www.kaggle.com/c/word2vec-nlp-tutorial) проходящего на сайте [Kaggle](https://www.kaggle.com) и представляют собой обучающую выборку из 25000 обзоров с сайта IMBD каждый из которых отнесё с одному из классов: негативный/позитивный. Задача предсказать к какому из классов будет относится каждый обзор из тестовой выборки.\n\n```{r}\nlibrary(magrittr)\nlibrary(tm)\nrequire(plyr)\nrequire(dplyr)\nlibrary(ggplot2)\nlibrary(randomForest)\n```\n\n\nЗагрузим данные в оперативную память.\n\n```{r cache=T}\ndata_train <- read.delim(\"labeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\n                               quote = \"\", stringsAsFactors = F)\n```\n\nПолученная таблица состоит из трёх столбцов: `id`, `sentiment` и `review`. Именно последний столбец и является объектом нашей работы. Посмотрим, что же из себя представляет сам обзор. (т.к. обзор достаточно длинный я приведу только первые 700 знаков)\n\n```{r}\npaste(substr(data_train[1,3],1,700),\"...\")\n```\n\nВидно, что в тексте присутствует мусор в виде HTML тегов. \n\n### Bag of Words\n\nBag of Words или мешок слов - это модель часто используемая при обработке текстов, представляющая собой неупорядоченный набор слов, входящих в обрабатываемый текст. Часто модель представляют в виде матрицы, в которой строки соответствуют отдельному тексту, а столбцы - входящие в него слова. Ячейки на пересечении являются числом вхождения данного слова в соответствующий документ. Данная модель удобна тем, что переводит человеческий язык слов в понятный для компьтера язык цифр. \n\n### Обработка данных.\n\nДля обработки данных я буду использовать возможности пакета `tm`. В следующем блоке кода производятся следующие действия:\n\n- создаётся вектор из текстов\n- создеётся корпус - коллекция текстов\n- все буквы приводятся к строчным\n- удаляются знаки пунктуации\n- удаляются так называемые \"стоп-слова\", т.к. часто встречающиеся слова в языке, не несущие сами по себе информации (в английском языке, например `and`) Кроме этого я решил сразу убрать слово, которое наверняка будет часто встречаться в обзорах, но интереса для модели не представляет - `movie`.\n- производится стеммирование, т.е. слова преобразуются в свою основную форму\n\n```{r cache=T}\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% \n        tm_map(., removeWords, c(\"movie\", stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\n        \n```\n\nТеперь создадим частотную матрицу.\n\n```{r cache=T}\nfrequencies <- DocumentTermMatrix(train_corpus)\nfrequencies\n```\n\nНаша матрица содержит более 90000 терминов, т.е. модель на её основе будет содержать 90000 признаков! Её необходимо уменьшать и для этого используем тот факт, что в ней очень много редко встречающихся в обзорах слов, т.е. она разряжена (термин `sparse`). Я решил её сократить очень сильно (для того, чтобы модель уместилась в оперативной памяти с учётом 25000 объектов в обучающей выборке) и оставить только те слова, что встречаются минимум в 5% обзоров.\n\n```{r cache=T}\nsparse <- removeSparseTerms(frequencies, 0.95)\nsparse\n```\n\nВ итоге в матрице осталось 373 термина. Преобразуем матрицу в `data frame` и добавим столбец с целевым признаком.\n\n```{r cache=T}\nreviewSparse = as.data.frame(as.matrix(sparse))\nvocab <- names(reviewSparse)\nreviewSparse$sentiment <- data_train$sentiment %>% as.factor(.) %>% \n        revalue(., c(\"0\"=\"neg\", \"1\" = \"pos\"))\nrow.names(reviewSparse) <- NULL\n```\n\nТеперь обучим Random Forest модель на полученных данных. Я использую 100 деревьев в связи с ограничением оперативной памяти.\n\n```{r cache=T}\nmodel_rf <- randomForest(sentiment ~ ., data = reviewSparse, ntree = 100)\n```\n\nИспользуя обученную модель создадим прогноз для тестовых данных.\n\n```{r cache=T}\ndata_test <- read.delim(\"testData.tsv\", header = TRUE, sep = \"\\t\",\n                               quote = \"\", stringsAsFactors = F)\ntest_corpus <- data_test$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% \n        tm_map(., removeWords, c(\"movie\", stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\ntest_frequencies <-  DocumentTermMatrix(test_corpus,control=list(dictionary = vocab))\nreviewSparse_test <-  as.data.frame(as.matrix(test_frequencies))\nrow.names(reviewSparse_test) <- NULL\nsentiment_test <- predict(model_rf, newdata = reviewSparse_test)\npred_test <- as.data.frame(cbind(data_test$id, sentiment_test))\ncolnames(pred_test) <- c(\"id\", \"sentiment\")\npred_test$sentiment %<>% revalue(., c(\"1\"=\"0\", \"2\" = \"1\"))\nwrite.csv(pred_test, file=\"Submission.csv\", quote=FALSE, row.names=FALSE)\n\n```\n\nПосле загрузки и оценки на сайте Kaggle модель получила оценку по статистике AUC - 0.73184. \n\nПопробуем подойти к проблеме с другой стороны. При составлении частотной матрицы и обрезании её мы оставляем наиболее часто встречающиеся слова, но, скорее всего, много слов, которые часто встречаются в обзорах фильмов, но не отражают настроение обзора. Например такие слова как `movie`, `film` и т.д. Но, т.к. у нас есть обучающая выборка с отмеченным настроением обзоров, можно выделить слова, частоты которых существенно различаются у негативных и положительных обзоров. \n\nДля начала, создадим частотную матрицу для негативных обзоров.\n\n```{r cache=T}\nfreq_neg <- data_train %>% filter(sentiment == 0) %>% select(review) %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>%\n        tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument) %>% DocumentTermMatrix(.) %>% \n        removeSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_neg <- colSums(freq_neg)\nfreq_df_neg <- data.frame(word = names(freq_df_neg), freq = freq_df_neg)\nrownames(freq_df_neg) <- NULL\nhead(arrange(freq_df_neg, desc(freq)))\n```\n\nИ для положительных обзоров.\n\n```{r cache=T}\nfreq_pos <- data_train %>% filter(sentiment == 1) %>% select(review) %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>%\n        tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument) %>% DocumentTermMatrix(.) %>% \n        removeSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_pos <- colSums(freq_pos)\nfreq_df_pos <- data.frame(word = names(freq_df_pos), freq = freq_df_pos)\nrownames(freq_df_pos) <- NULL \nhead(arrange(freq_df_pos, desc(freq)))\n```\n\nОбъединим полученные таблицы и посчитаем разницу между частотами.\n\n```{r cache=T}\nfreq_all <- merge(freq_df_neg, freq_df_pos, by = \"word\", all = T)\nfreq_all$freq.x[is.na(freq_all$freq.x)] <- 0\nfreq_all$freq.y[is.na(freq_all$freq.y)] <- 0\nfreq_all$diff <- abs(freq_all$freq.x - freq_all$freq.y)\nhead(arrange(freq_all, desc(diff)))\n```\n\nОтлично! Мы видим, как и ожидалось, среди слов с наибольшей разницей такие термины как `bad`, `great` и `love`. Но также здесь и просто часто встречающиеся слова, как `movie`. Это произошло, что у частых слов даже небольшая процентная разница выдаёт высокую абсолютную разницу. Для того, чтобы устранить это упущение, нормализуем разницу, разделив её на сумму частот. Получившаяся метрика будет лежать в интервале между 0 и 1, и чем выше её значение - тем важнее данное значение в определении разницы между положительными и отрицательными отзывами. Но что же делать со словами, которые встречаются только у одного класса отзывов и при этом их частота мала? Для уменьшения их важности добавим к знаменателю коэффициент.\n\n```{r cache=T}\nfreq_all$diff_norm <- abs(freq_all$freq.x - freq_all$freq.y)/\n        (freq_all$freq.x +freq_all$freq.y + 300)\nhead(arrange(freq_all, desc(diff_norm)))\n```\n\nОтберём 500 слов с наивысшим показателем коэффициента разницы.\n\n```{r cache=T}\nfreq_word <- arrange(freq_all, desc(diff_norm)) %>% select(word) %>% slice(1:500)\n```\n\nИспользуем полученный словарь для создания частотной матрицы, на которой обучим Random Forest модель.\n\n```{r cache=T}\nvocab <- as.character(freq_word$word)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab))\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\nrow.names(reviewSparse_train) <- NULL\nreviewSparse_train$sentiment <- data_train$sentiment %>% as.factor(.) %>%\nrevalue(., c(\"0\"=\"neg\", \"1\" = \"pos\"))\n\nmodel_rf <- randomForest(sentiment ~ ., data = reviewSparse_train, ntree = 100)\n\n```\n\nПосле загрузки и оценки на сайте Kaggle модель получила оценку по статистике AUC - 0.83120, т.е. поработав с признаками мы получили улучшение статистики на 10%!\n\n### TF-IDF\n\nПри создании матрицы документ-термин в качестве метрики важности слова мы использовали просто частоту появления слова в обзоре. В пакете `tm` есть возможность использовать другую меру, называемую tf-idf. TF-IDF (от англ. TF — term frequency, IDF — inverse document frequency) — статистическая метрика, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Вес некоторого слова пропорционален количеству употребления этого слова в документе, и обратно пропорционален частоте употребления слова в других документах коллекции. \n\nИспользуя tf-idf, создадим словарь из 500 терминов с наиболее высоким показателем данной метрики. Для того, чтобы этот словарь наиболее релевантно отражал важность слов, будем использовать дополнительную обучающую выборку, в которой не размечено настроение обзоров. На базе полученного словаря создадим матрицу документ-термин и обучим модель.\n\n```{r, eval=FALSE}\ndata_train_un <- read.delim(\"unlabeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\n                            quote = \"\", stringsAsFactors = F)\ntrain_review <- c(data_train$review, data_train_un$review)\ntrain_corpus <- train_review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\ntdm <- TermDocumentMatrix(train_corpus,\n                          control = list(weighting = function(x) weightTfIdf(x, normalize = F)))\nlibrary(slam)\nfreq <- rollup(tdm, 2,FUN = sum)\nfreq <- as.matrix(freq)\nfreq_df <- data.frame(word = row.names(freq), tfidf = freq)\nnames(freq_df) <- c(\"word\", \"tf_idf\")\nrow.names(freq_df) <- NULL\nfreq_df %<>% arrange(desc(tf_idf))\nvocab <- as.character(freq_df$word)[1:500]\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\n                                                           weighting = function(x) weightTfIdf(x, normalize = F) ))\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\nrm(data_train_un, tdm, dtm, train_review)\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\nrow.names(reviewSparse_train) <- NULL\ncolnames(reviewSparse_train) = make.names(colnames(reviewSparse_train))\nreviewSparse_train$sentiment <- data_train$sentiment %>% as.factor(.) %>%\nrevalue(., c(\"0\"=\"neg\", \"1\" = \"pos\"))\nrm(data_train, train_corpus, freq, freq_df)\nmodel_rf <- randomForest(sentiment ~ ., data = reviewSparse_train, ntree = 100)\n\n```\n\nИспользуем данную модель на тестовой выборке и получим значение AUC - 0.81584.\n\n### Заключение.\n\nДанная работа представляет собой один из возможных вариантов создания предсказательной модели на основе текстовых данных. Одним из вариантов улучшить качество модели может быть увеличение количества используемых терминов из матрицы документ-термин, но этот путь требует существенного увеличения используемых машинных ресурсов. Также может привести к гораздо лучшим результатам обратиться не к частотам слов, а к их значениям и связям между ними. Для этого надо обратиться к модели `word2vec`. Кроме этого, большое поле для исследования представляет собой рассмотрение терминов в контексте документа.\n\n\n\n",
    "created" : 1441220560167.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3740814497",
    "id" : "9C5BF624",
    "lastKnownWriteTime" : 1441281693,
    "path" : "G:/R lang/Kaggle_Sentiment_Analisys/Sentiment Analisys.Rmd",
    "project_path" : "Sentiment Analisys.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}