{
    "contents" : "data_test <- read.delim(\"testData.tsv\", header = TRUE, sep = \"\\t\",\nquote = \"\", stringsAsFactors = F)\ntest_corpus <- data_test$review %>% VectorSource(.)%>%\nCorpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\ntm_map(., removePunctuation) %>%\ntm_map(., removeWords, c(stopwords(\"english\"))) %>%\ntm_map(., stemDocument)\ntest_frequencies <-  DocumentTermMatrix(test_corpus,control=list(dictionary = vocab, \n                                                                 weighting = function(x) weightTfIdf(x, normalize = F)))\nreviewSparse_test <-  as.data.frame(as.matrix(test_frequencies))\nrow.names(reviewSparse_test) <- NULL\nsentiment_test <- predict(model_rf, newdata = reviewSparse_test)\nrm(test_corpus)\nrm(train_corpus)\nsentiment_test <- predict(model_rf, newdata = reviewSparse_test)\npred_test <- as.data.frame(cbind(data_test$id, sentiment_test))\ncolnames(pred_test) <- c(\"id\", \"sentiment\")\npred_test$sentiment %<>% revalue(., c(\"1\"=\"0\", \"2\" = \"1\"))\nwrite.csv(pred_test, file=\"Submission.csv\", quote=FALSE, row.names=FALSE)\n\n\ndata_train <- read.delim(\"labeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\nquote = \"\", stringsAsFactors = F)\ndata_train_un <- read.delim(\"unlabeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\nquote = \"\", stringsAsFactors = F)\ntrain_review <- c(data_train$review, data_train_un$review)\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\n\n\ntdm <- TermDocumentMatrix(train_corpus,\ncontrol = list(weighting = function(x) weightTfIdf(x, normalize = F)))\nlibrary(slam)\nfreq <- rollup(tdm, 2,FUN = sum)\nfreq\nfreq <- as.matrix(freq)\nfreq_df <- data.frame(word = row.names(freq), tfidf = freq)\nnames(freq_df) <- c(\"word\", \"tf_idf\")\nrow.names(freq_df) <- NULL\n\ndata_train <- read.delim(\"labeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\nquote = \"\", stringsAsFactors = F)\nfreq_neg <- data_train %>% filter(sentiment == 0) %>% select(review) %>% VectorSource(.)%>%\nCorpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\ntm_map(., removePunctuation) %>%\ntm_map(., removeNumbers) %>%\ntm_map(., removeWords, c(stopwords(\"english\"))) %>%\ntm_map(., stemDocument) %>% DocumentTermMatrix(.) %>%\nremoveSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_neg <- colSums(freq_neg)\nfreq_df_neg <- data.frame(word = names(freq_df_neg), freq = freq_df_neg)\nrownames(freq_df_neg) <- NULL\nhead(arrange(freq_df_neg, desc(freq)))\nfreq_pos <- data_train %>% filter(sentiment == 1) %>% select(review) %>% VectorSource(.)%>%\nCorpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\ntm_map(., removePunctuation) %>%\ntm_map(., removeNumbers) %>%\ntm_map(., removeWords, c(stopwords(\"english\"))) %>%\ntm_map(., stemDocument) %>% DocumentTermMatrix(.) %>%\nremoveSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_pos <- colSums(freq_pos)\nfreq_df_pos <- data.frame(word = names(freq_df_pos), freq = freq_df_pos)\nrownames(freq_df_pos) <- NULL\nhead(arrange(freq_df_pos, desc(freq)))\nfreq_all <- merge(freq_df_neg, freq_df_pos, by = \"word\", all = T)\nfreq_all$freq.x[is.na(freq_all$freq.x)] <- 0\nfreq_all$freq.y[is.na(freq_all$freq.y)] <- 0\nfreq_all$diff <- abs(freq_all$freq.x - freq_all$freq.y)\nhead(arrange(freq_all, desc(diff)))\nfreq_all$diff_norm <- abs(freq_all$freq.x - freq_all$freq.y)/\n(freq_all$freq.x +freq_all$freq.y + 300)\nhead(arrange(freq_all, desc(diff_norm)))\nfreq_word <- arrange(freq_all, desc(diff_norm)) %>% select(word) %>% slice(1:500)\nvocab <- as.character(freq_word$word)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\nweighting = function(x) weightTfIdf(x, normalize = F) ))\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\nCorpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\ntm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\ntm_map(., removeWords, c(stopwords(\"english\"))) %>%\ntm_map(., stemDocument)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\nweighting = function(x) weightTfIdf(x, normalize = F) ))\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\nrow.names(reviewSparse_train) <- NULL\nreviewSparse_train$sentiment <- data_train$sentiment %>% as.factor(.) %>%\nrevalue(., c(\"0\"=\"neg\", \"1\" = \"pos\"))\nmodel_rf <- randomForest(sentiment ~ ., data = reviewSparse_train, ntree = 100, do.trace = T)\ndata_test <- read.delim(\"testData.tsv\", header = TRUE, sep = \"\\t\",\nquote = \"\", stringsAsFactors = F)\ntest_corpus <- data_test$review %>% VectorSource(.)%>%\nCorpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\ntm_map(., removePunctuation) %>%\ntm_map(., removeWords, c(stopwords(\"english\"))) %>%\ntm_map(., stemDocument)\ntest_frequencies <-  DocumentTermMatrix(test_corpus,control=list(dictionary = vocab,\nweighting = function(x) weightTfIdf(x, normalize = F)))\nreviewSparse_test <-  as.data.frame(as.matrix(test_frequencies))\nrow.names(reviewSparse_test) <- NULL\nsentiment_test <- predict(model_rf, newdata = reviewSparse_test)\npred_test <- as.data.frame(cbind(data_test$id, sentiment_test))\ncolnames(pred_test) <- c(\"id\", \"sentiment\")\npred_test$sentiment %<>% revalue(., c(\"1\"=\"0\", \"2\" = \"1\"))\nwrite.csv(pred_test, file=\"Submission.csv\", quote=FALSE, row.names=FALSE)\n\n\ndata_train <- read.delim(\"labeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\n                         quote = \"\", stringsAsFactors = F)\nfreq_neg <- data_train %>% filter(sentiment == 0) %>% select(review) %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>%\n        tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument) %>% \n        DocumentTermMatrix(.,control=list(weighting = function(x) weightTfIdf(x, normalize = F))) %>%\n        removeSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_neg <- colSums(freq_neg)\nfreq_df_neg <- data.frame(word = names(freq_df_neg), freq = freq_df_neg)\nrownames(freq_df_neg) <- NULL\nhead(arrange(freq_df_neg, desc(freq)))\nfreq_pos <- data_train %>% filter(sentiment == 1) %>% select(review) %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>%\n        tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument) %>% DocumentTermMatrix(.) %>%\n        removeSparseTerms(., 0.999) %>% as.matrix(.)\nfreq_df_pos <- colSums(freq_pos)\nfreq_df_pos <- data.frame(word = names(freq_df_pos), freq = freq_df_pos)\nrownames(freq_df_pos) <- NULL\nhead(arrange(freq_df_pos, desc(freq)))\nfreq_all <- merge(freq_df_neg, freq_df_pos, by = \"word\", all = T)\nfreq_all$freq.x[is.na(freq_all$freq.x)] <- 0\nfreq_all$freq.y[is.na(freq_all$freq.y)] <- 0\nfreq_all$diff <- abs(freq_all$freq.x - freq_all$freq.y)\nhead(arrange(freq_all, desc(diff)))\nfreq_all$diff_norm <- abs(freq_all$freq.x - freq_all$freq.y)/\n        (freq_all$freq.x +freq_all$freq.y + 300)\nhead(arrange(freq_all, desc(diff_norm)))\nfreq_word <- arrange(freq_all, desc(diff_norm)) %>% select(word) %>% slice(1:500)\nvocab <- as.character(freq_word$word)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\n                                                           weighting = function(x) weightTfIdf(x, normalize = F) ))\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\n                                                           weighting = function(x) weightTfIdf(x, normalize = F) ))\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\nrow.names(reviewSparse_train) <- NULL\nreviewSparse_train$sentiment <- data_train$sentiment %>% as.factor(.) %>%\n        revalue(., c(\"0\"=\"neg\", \"1\" = \"pos\"))\nmodel_rf <- randomForest(sentiment ~ ., data = reviewSparse_train, ntree = 100, do.trace = T)\ndata_test <- read.delim(\"testData.tsv\", header = TRUE, sep = \"\\t\",\n                        quote = \"\", stringsAsFactors = F)\ntest_corpus <- data_test$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\ntest_frequencies <-  DocumentTermMatrix(test_corpus,control=list(dictionary = vocab,\n                                                                 weighting = function(x) weightTfIdf(x, normalize = F)))\nreviewSparse_test <-  as.data.frame(as.matrix(test_frequencies))\nrow.names(reviewSparse_test) <- NULL\nsentiment_test <- predict(model_rf, newdata = reviewSparse_test)\npred_test <- as.data.frame(cbind(data_test$id, sentiment_test))\ncolnames(pred_test) <- c(\"id\", \"sentiment\")\npred_test$sentiment %<>% revalue(., c(\"1\"=\"0\", \"2\" = \"1\"))\nwrite.csv(pred_test, file=\"Submission.csv\", quote=FALSE, row.names=FALSE)\n\n\n\n\n\ndata_train_un <- read.delim(\"unlabeledTrainData.tsv\",header = TRUE, sep = \"\\t\",\n                            quote = \"\", stringsAsFactors = F)\ntrain_review <- c(data_train$review, data_train_un$review)\ntrain_corpus <- train_review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\ntdm <- TermDocumentMatrix(train_corpus,\n                          control = list(weighting = function(x) weightTfIdf(x, normalize = F)))\nfreq <- rollup(tdm, 2,FUN = sum)\nfreq <- as.matrix(freq)\nfreq_df <- data.frame(word = row.names(freq), tfidf = freq)\nnames(freq_df) <- c(\"word\", \"tf_idf\")\nrow.names(freq_df) <- NULL\nfreq_df %<>% arrange(desc(tf_idf))\nvocab <- as.character(freq_df$word)[1:500]\ntrain_corpus <- data_train$review %>% VectorSource(.)%>%\n        Corpus(.) %>% tm_map(., tolower) %>% tm_map(., PlainTextDocument) %>%\n        tm_map(., removePunctuation) %>% tm_map(., removeNumbers) %>%\n        tm_map(., removeWords, c(stopwords(\"english\"))) %>%\n        tm_map(., stemDocument)\nfrequencies = DocumentTermMatrix(train_corpus,control=list(dictionary = vocab,\n                                                           weighting = function(x) weightTfIdf(x, normalize = F) ))\nreviewSparse_train <-  as.data.frame(as.matrix(frequencies))\n",
    "created" : 1441220603673.000,
    "dirty" : true,
    "encoding" : "",
    "folds" : "",
    "hash" : "389713479",
    "id" : "632C5B88",
    "lastKnownWriteTime" : 578055781,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}